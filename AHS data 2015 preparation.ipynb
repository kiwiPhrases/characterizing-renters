{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "jet = plt.get_cmap('jet')\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from statsmodels.regression.linear_model import WLS\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "from scipy.stats import norm\n",
    "\n",
    "from statsmodels.discrete.discrete_model import Poisson\n",
    "\n",
    "import itertools\n",
    "data_path = \"C:/Users/SpiffyApple/Documents/USC/OwnResearch/proposal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convenience function\n",
    "def toLatex(tmpDF, file_name):\n",
    "    with open(\"/\".join([data_path,'output',file_name]), 'w+') as f:\n",
    "        f.write(tmpDF.to_latex())\n",
    "    return(tmpDF)\n",
    "\n",
    "def toExcel(tmpDF, file_name):\n",
    "    tmpDF.to_excel(\"/\".join([data_path,'output',file_name]))\n",
    "    return(tmpDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data come from [here](https://www.census.gov/programs-surveys/ahs/data/2015/ahs-2015-public-use-file--puf-/2015-ahs-metropolitan-puf-microdata.html). Important AHS data documentation [here](https://www.census.gov/content/dam/Census/programs-surveys/ahs/tech-documentation/2015/Getting%20Started%20with%20the%20AHS%20PUF.pdf), variable [labels](https://www.census.gov/data-tools/demo/codebook/ahs/ahsdict.html?s_appName=ahsdict&s_year=2015)\n",
    "\n",
    "Other years include:, [2017]()\n",
    "[2013](https://www.census.gov/programs-surveys/ahs/data/2013/ahs-2013-public-use-file--puf-/ahs-2013-national-public-use-file--puf-.html), and \n",
    "[2011](https://www.census.gov/programs-surveys/ahs/data/2011/ahs-national-and-metropolitan-puf-microdata.html)\n",
    "\n",
    "Another useful link is the [QuickFacts](https://www.census.gov/quickfacts/fact/table/milwaukeecitywisconsin/PST045217) for Milwaukee which gives basic information on housing and demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AHS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.read_csv(\"/\".join([data_path, 'ahs2015m.csv']))\n",
    "dfn = pd.read_csv(\"/\".join([data_path, '2015householdNational.csv']))\n",
    "labels = pd.read_csv(\"/\".join([data_path, 'ahs 2015 value labels.csv']), encoding='latin-1')\n",
    "labels.drop(['Table','Type','FLAT','METRO',\"FMTName\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SpiffyApple\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94379, 1091)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge the national and metropolitan files\n",
    "df = pd.concat([dfm.loc[:,dfm.columns.intersection(dfn.columns)],dfn]) \n",
    "df.replace(\"'\",'',regex=True,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94379, 1013)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up the data\n",
    "df.replace(\"'\",'',regex=True,inplace=True) #remove \"'\" marks to convert to numerical\n",
    "df = df.astype(np.float) #convert all to float - useful because it clears up the issue of entries such as '01'\n",
    "df.replace(-9,np.nan, inplace=True) #replace -9s with np.nan\n",
    "\n",
    "df.drop(df.columns[df.isnull().all()], axis=1, inplace=True) #drop columns that contain all NULLs\n",
    "\n",
    "df.drop(df.columns[df.isnull().sum()/df.shape[0]>.9], axis=1, inplace=True) #drop columns in which more than 90% of the data are NULLs\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Labels to AHS Data values\n",
    "\n",
    "Labels file comes from the Label Values file [here](https://www.census.gov/programs-surveys/ahs/data/2015/ahs-2015-public-use-file--puf-/ahs-2015-national-public-use-file--puf-.html). However, this file is only used to replace the values. \n",
    "\n",
    "I also downloaded the entire codebook from [AHS Codebook](https://www.census.gov/data-tools/demo/codebook/ahs/ahsdict.html) that details a lot more information than the label values has. It is also a convenient way to search for topics and the like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## upload labels data\n",
    "labels = pd.read_csv(\"/\".join([data_path, 'ahs 2015 value labels.csv']), encoding='latin-1')\n",
    "labels.drop(['Table','Type','FLAT','METRO',\"FMTName\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.Value.replace(\".N|.M|N|M\",-9,regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pandas can only replace values in a structured manner through dictionaries\n",
    "## to do that, I set values as indexes and convert the remaining Series\n",
    "## into a dictionary for each column name\n",
    "## there's the added complicationg of needing to compare indexes of same type\n",
    "\n",
    "labels.set_index('Value',inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reNameDict = {}\n",
    "for label, group in labels.groupby(\"NAME\"):\n",
    "    if label in df.columns:\n",
    "        if np.issubdtype(np.object,df.loc[:,label].dtype ):\n",
    "            reNameDict[label] = group.Label.to_dict()\n",
    "        else:\n",
    "            group.index = group.index.astype(df.loc[:,label].dtype)\n",
    "            reNameDict[label] = group.Label.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(reNameDict,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(-9,np.nan, inplace=True)\n",
    "df.replace(-6,np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94379, 1012)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df.columns[df.isnull().all()], axis=1, inplace=True) #drop columns that contain all NULLs\n",
    "\n",
    "df.drop(df.columns[df.isnull().sum()/df.shape[0]>.9], axis=1, inplace=True) #drop columns in which more than 90% of the data are NULLs\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels for columns\n",
    "ahsdict = pd.read_csv(\"/\".join([data_path, 'AHSDICT_28AUG18_23_35_13_43_S.csv']))\n",
    "#drop unnecessary columns\n",
    "ahsdict.drop(['Disclosure','Variable_Number','INUNIVERSE','Topic_Number','Present_in_Survey_Years','Topic_Subtopic_ID','Imputation_Strategy','Question','Instrument_Variable_Name','Survey Year','Subtopic'],axis=1,inplace=True)\n",
    "ahsdict.set_index('Variable',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[df.columns.str.lower().str.contains(\"wgt|weight\")], axis=1,inplace=True) #drop weight cols\n",
    "#df.drop(\"Unnamed: 0\", axis=1, inplace=True) # drop unnamed col\n",
    "df.drop(df.columns[df.isnull().all()], axis=1, inplace=True) #drop cols that are all null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename variable names to their labels\n",
    "#df.rename(columns =  ahsdict.Description.str.replace(\"\\s|:|,|^\\d+|\\([\\w\\s\\-?]+\\)\",'').str.lower().to_dict(), inplace=True)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a convenience function for variable look up\n",
    "def varlu(s):\n",
    "    return(ahsdict[ahsdict.Description.str.lower().str.contains(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Data \n",
    "\n",
    "Crosswalk data for CBSA to everything else comes from [NBER](http://www.nber.org/data/cbsa-fips-county-crosswalk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load geographc data\n",
    "fips= pd.read_csv(\"/\".join([data_path, 'cbsa2fipsxw.csv']), encoding='latin-1')\n",
    "fips.dropna(how='all',inplace=True)\n",
    "fips = fips[['cbsacode','statename','cbsatitle','csatitle','countycountyequivalent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of CBSA codes in crosswalk file 1882\n",
      "Num of unique CBSA codes in crosswalk: 929\n"
     ]
    }
   ],
   "source": [
    "# CBSA codes do not map uniquely to counties which is a problem \n",
    "# because when merging with the main dataset, it matched one to many\n",
    "# thereby substantially overinflating the number of counties and data\n",
    "# that are actually in the dataset. \n",
    "print(\"Num of CBSA codes in crosswalk file\", fips.cbsacode.shape[0])\n",
    "print(\"Num of unique CBSA codes in crosswalk:\", fips.cbsacode.unique().shape[0] )\n",
    "\n",
    "# by dropping duplicatec CBSA codes, I eliminate this issue\n",
    "# though which county of the many counties mapped to a single CBSA code\n",
    "# gets mapped into the main dataset is nigh random or arbitrary\n",
    "fips.drop_duplicates(subset=['cbsacode', 'csatitle'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure cbsacode field matched the type in the mother dataframe:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cbsacode                  float64\n",
       "statename                  object\n",
       "cbsatitle                  object\n",
       "csatitle                   object\n",
       "countycountyequivalent     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Make sure cbsacode field matched the type in the mother dataframe:\")\n",
    "fips.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.OMB13CBSA.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset with CBSAs available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(62733, 533)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## merge with housing data\n",
    "df = df.merge(fips, left_on = 'OMB13CBSA', right_on = 'cbsacode', how='left')\n",
    "\n",
    "### drop if we do not know the metro area\n",
    "df = df.loc[df.cbsatitle.notnull(),:]\n",
    "\n",
    "print(\"Size of dataset with CBSAs available\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Scrub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset once observations for which ownership/renter status is not known\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55834, 533)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## drop observations for which we do not know renter status\n",
    "df.dropna(subset=['TENURE'], inplace=True)\n",
    "print(\"Size of dataset once observations for which ownership/renter status is not known\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55834, 234)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## drop EDIT variables\n",
    "df = df.loc[:,~df.columns.str.contains(\"^J\")]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop mobile home and trailer observations\n",
    "df = df.loc[df.BLD != 'Mobile home or trailer']\n",
    "df = df[df.BLD != 'Boat, RV, van, etc.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.BLD != 'Boat, RV, van, etc.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUmber of observations in full dataset without a full bath: 0\n",
      "How many rental units are there with more than 3 bathrooms in each cbsa?\n"
     ]
    }
   ],
   "source": [
    "print(\"NUmber of observations in full dataset without a full bath: %d\" %(df.BATHROOMS =='noFullBath').sum())\n",
    "df = df.loc[df.BATHROOMS !='noFullBath'] # drop observations without a full bath\n",
    "\n",
    "print(\"How many rental units are there with more than 3 bathrooms in each cbsa?\")\n",
    "df.loc[(df.BATHROOMS == 'More than 3 bathrooms') & (df.TENURE =='Rented'),['cbsatitle','HINCP']].groupby('cbsatitle').size()\n",
    "\n",
    "df = df.loc[df.BATHROOMS != 'More than 3 bathrooms'] ## drop obvservations with 3 bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop observations with 0 incomes and 0 rents\n",
    "df = df[df.HINCP > 0]\n",
    "df = df[(df.RENT > 0) & df.RENT.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convenience function for making dummy variables\n",
    "def makeVar(newVar,oldVar,entry,data=df):\n",
    "    df.loc[:,newVar] = (df.loc[:,oldVar] == entry).astype(np.float)\n",
    "    df.loc[df[oldVar].isnull(), newVar] = np.nan\n",
    "    \n",
    "def makeDummies(col,makeTitle=True,df = df):\n",
    "    dummies = pd.get_dummies(df.loc[:,col].replace(-6,np.nan))\n",
    "    dummies =dummies.loc[:,[x for x in dummies.columns if x is not np.nan]]\n",
    "    if makeTitle:\n",
    "        dummies.columns = [col+s for s in dummies.columns.str.replace(\"-|\\s|,|:|\\.\",\"\")]\n",
    "    if makeTitle is False:\n",
    "        dummies.columns = dummies.columns.str.replace(\"-|\\s|,|:|\\.\",\"\")\n",
    "    dummies.loc[df[col].isnull()]  = np.nan\n",
    "    #(dummies==0).sum()/dummies.shape[0]\n",
    "    return(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'logincome'] = np.log(df.HINCP+1)\n",
    "df.loc[:,'incomeSqrd'] = df.HINCP**2\n",
    "df.loc[:,'hhincomeper1000'] = df.HINCP/1e3\n",
    "df.loc[:,'hhagesqrd'] = df.HHAGE**2\n",
    "df.loc[:,'logRent']= np.log(df.RENT)\n",
    "df.loc[:,'lengthTenure'] = 2015 - df.HHMOVE\n",
    "df.loc[:,'collegeOrMore'] = df.HHGRAD.isin(['Professional School Degree (For Ex: MD, DDS, DVM, LLB, JD)','Doctorate Degree (For Ex: PhD, EdD)',\n",
    "                                           'Master\\x92s Degree (For Ex: MA, MS, Meng, Med, MSW, MBA)','Bachelor\\x92s degree (For Ex: BA, AB, BS)'])\n",
    "df.loc[:,'collegeOrMore'] = df.collegeOrMore.astype(np.int32)\n",
    "df['HHRACE'].replace(\" Only| / | \",\"\",regex=True,inplace=True)\n",
    "df.loc[:,'femaleHH'] = (df.HHSEX == 'Female').astype(np.float)\n",
    "df.loc[:,'marriedHH'] = (df.HHMAR == 'Married, spouse present').astype(np.float)\n",
    "df.loc[:,'badSchools'] = (df.NHQSCHOOL == 'Disagree').astype(np.float)\n",
    "df.loc[:,'lotCrime'] = (df.NHQSCRIME == 'Agree').astype(np.float);\n",
    "makeVar('secured','ENTRYSYS','Yes')\n",
    "makeVar('bars','WINBARS','Yes')\n",
    "makeVar('washer','WASHER','Yes')\n",
    "makeVar('fridge','FRIDGE','Yes')\n",
    "makeVar('feltCold','COLD','Yes')\n",
    "makeVar('notoilet','NOTOIL','Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a dummy on weather the unit is professionally managed\n",
    "df.loc[:,'profMngd'] = df.MGRONSITE.isin(['2-4 unit building has owner on-site',\n",
    "       '5 or more unit building has owner and/or manager on-site',\n",
    "       '2-4 unit building has manager on-site'])\n",
    "df.loc[df.MGRONSITE.isnull(),'profMngd'] = np.nan ## mark NAs\n",
    "\n",
    "## make dummy for roaches\n",
    "df.loc[:,'roaches'] = df.ROACH.isin(['Seen daily in the last 12 months',\n",
    "       'Seen a few times in the last 12 months',\n",
    "       'Seen weekly in the last 12 months',\n",
    "       'Seen monthly in the last 12 months'])\n",
    "df.loc[df.ROACH.isnull(),'roaches'] = np.nan\n",
    "\n",
    "## make dummy for roadents\n",
    "df.loc[:,'rodents'] = df.RODENT.isin(['Seen daily in the last 12 months',\n",
    "       'Seen a few times in the last 12 months',\n",
    "       'Seen weekly in the last 12 months',\n",
    "       'Seen monthly in the last 12 months'])\n",
    "df.loc[df.RODENT.isnull(),'rodents'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make race variables\n",
    "df = pd.concat([df, pd.get_dummies(df.ADEQUACY)[['Moderately Inadequate', 'Severely Inadequate']]],axis=1)\n",
    "df.rename(columns = {\"Moderately Inadequate\":'moderatelyInadequate','Severely Inadequate':'severelyInadequate'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share of sample that is either black, white, or asian: 0.97\n"
     ]
    }
   ],
   "source": [
    "## make race variables\n",
    "df = pd.concat([df, pd.get_dummies(df.HHRACE)[['Black','White','Asian']]],axis=1)\n",
    "print(\"Share of sample that is either black, white, or asian: %.2f\" %(df[['Black','White','Asian']].sum()/df.shape[0]).sum())\n",
    "\n",
    "df.loc[:,'OtherRace'] = ((df.Black == 0) & (df.White==0) & (df.Asian == 0)) #.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to remove outliers and the like, take a broad swipe \n",
    "## exclude observations below and above the 10th and 90th percentiles respectively for rent and incomes\n",
    "\n",
    "rentPercentiles = [.025,.05,.1,.8,.9,.975]\n",
    "rentBounds = df.loc[:,['cbsatitle', 'RENT']].dropna().groupby('cbsatitle').quantile(rentPercentiles).unstack(level=1)\n",
    "rentBounds.columns = rentBounds.columns.droplevel(level=0)\n",
    "rentBounds.columns = [\"rent\"+str(s) for s in rentBounds.columns]\n",
    "\n",
    "incPercentiles = [.05,.1,.8,.9]\n",
    "incBounds = df.loc[:,['cbsatitle', 'HINCP']].dropna().groupby('cbsatitle').quantile(incPercentiles).unstack(level=1)\n",
    "incBounds.columns = incBounds.columns.droplevel(level=0)\n",
    "incBounds.columns = [\"inc\"+str(s) for s in incBounds.columns]\n",
    "df = df.merge(pd.concat([incBounds, rentBounds], axis=1), right_index=True, left_on = 'cbsatitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dummies:\n",
    "\n",
    "## fix some variables before dumping them into creation of dummies\n",
    "df.loc[:,'BATHROOMS'] = df.BATHROOMS.str.replace(\"No full bath:[\\s\\w,?]+\",'noFullBath')\n",
    "### add columns out of which I would like to make dummies\n",
    "toDummiesList = ['BATHROOMS','BLD','YRBUILT','GARAGE',\n",
    "                 'PORCH','UNITSIZE']\n",
    "### make the dummies\n",
    "dummylist = []\n",
    "for i,col in enumerate(toDummiesList):\n",
    "    #print(col)\n",
    "    dummylist.append(makeDummies(col,df=df))\n",
    "\n",
    "dummies =  pd.concat(dummylist, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies.drop(['GARAGENo','PORCHNo'],axis=1,inplace=True)\n",
    "dummies.columns = dummies.columns.str.replace(\"squarefeet\",'sqft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is tenure dummy perfect?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[df.TENURE != 'Occupied without payment of rent'] ## exclude units with odd status\n",
    "\n",
    "df.loc[:,'renter'] = (df.TENURE == 'Rented').astype(int)\n",
    "df.loc[:,'owner']  = (df.TENURE == 'Owned or being bought by someone in your household').astype(int)\n",
    "\n",
    "print(\"Is tenure dummy perfect?\")\n",
    "df.renter.sum() + df.owner.sum() == df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"/\".join([data_path, 'AHS2015prepped.csv']))\n",
    "df.to_csv(\"/\".join([data_path, 'AHS2015prepped_ver2.csv']), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
